{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dronesimscape import Dronesimscape\n",
    "from simulink_gym import logger\n",
    "\n",
    "logger.setLevel(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Adding components to Matlab path\n",
      "INFO: Creating simulation input object for model Dronesimscape.slx\n"
     ]
    }
   ],
   "source": [
    "# Create training environment:\n",
    "env = Dronesimscape(stop_time=10, step_size=0.001, timestep=0.001, model_debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: recv_socket: Socket not connected, nothing to close\n",
      "INFO: send_socket: Socket not connected, nothing to close\n"
     ]
    }
   ],
   "source": [
    "# Reset environment:\n",
    "state = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute single step in the environment:\n",
    "# import numpy as np\n",
    "# desired_pose = np.array([0, 0, 2], dtype=np.float32)\n",
    "# axispose = state[0]\n",
    "# axisvel = state[1]\n",
    "# platformpose = state[2]\n",
    "# pose = np.array([state[3], state[5], state[7]])\n",
    "# vel = np.array([state[4], state[6], state[8]])\n",
    "# omega = np.array([state[9], state[10], state[11]])\n",
    "# angular = np.array([state[12], state[13], state[14]])\n",
    "# yawrate = state[15]\n",
    "# current_z = state[7]\n",
    "# action = env.action_space.sample()\n",
    "# np.linalg.norm(action[2:]) * 0.00001\n",
    "# np.linalg.norm(angular)\n",
    "# np.linalg.norm(desired_pose - pose) * 0.01\n",
    "# np.linalg.norm(omega)\n",
    "# state, reward, done, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = env.action_space.sample()\n",
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Device set to : NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from agent.PPOAgent import *\n",
    "total_test_episodes = 10    # total num of testing episodes\n",
    "\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003           # learning rate for actor\n",
    "lr_critic = 0.001           # learning rate for critic\n",
    "env_name = 'Dronesimscape'\n",
    "directory = \"agent/PPO_preTrained/\" + env_name + '/'\n",
    "checkpoint_path = directory + \"PPO_{}.pth\".format(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "training environment name : Dronesimscape\n",
      "save checkpoint path : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  100000000\n",
      "max timesteps per episode :  5000\n",
      "model saving frequency : 20000 timesteps\n",
      "log frequency : 10000 timesteps\n",
      "printing average reward over episodes in last : 20000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a continuous action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "starting std of action distribution :  None\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 20000 timesteps\n",
      "PPO K epochs :  40\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Started training at (GMT) :  2023-12-12 10:39:23\n",
      "============================================================================================\n",
      "Episode : 3 \t\t Timestep : 20000 \t\t Average Reward : -41474.75\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:06:03\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 7 \t\t Timestep : 40000 \t\t Average Reward : -25322.91\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:12:05\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 11 \t\t Timestep : 60000 \t\t Average Reward : -24965.24\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:18:02\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 15 \t\t Timestep : 80000 \t\t Average Reward : -15244.33\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:23:53\n",
      "--------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.099\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 19 \t\t Timestep : 100000 \t\t Average Reward : -20657.05\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:29:40\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 23 \t\t Timestep : 120000 \t\t Average Reward : -29570.11\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:35:21\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 27 \t\t Timestep : 140000 \t\t Average Reward : -9957.5\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:41:05\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 31 \t\t Timestep : 160000 \t\t Average Reward : -21846.96\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:46:47\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 35 \t\t Timestep : 180000 \t\t Average Reward : -20769.25\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:52:30\n",
      "--------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.098\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 39 \t\t Timestep : 200000 \t\t Average Reward : -10087.03\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:58:17\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 43 \t\t Timestep : 220000 \t\t Average Reward : -14769.37\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  1:04:00\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 47 \t\t Timestep : 240000 \t\t Average Reward : -10806.76\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  1:09:50\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 51 \t\t Timestep : 260000 \t\t Average Reward : -11682.04\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  1:15:42\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 55 \t\t Timestep : 280000 \t\t Average Reward : -28755.02\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  1:21:30\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 159\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_ep_len\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    156\u001b[0m     \n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# select action with policy\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     action \u001b[38;5;241m=\u001b[39m ppo_agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m--> 159\u001b[0m     state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     pose \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([state[\u001b[38;5;241m3\u001b[39m], state[\u001b[38;5;241m5\u001b[39m], state[\u001b[38;5;241m7\u001b[39m]])\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(env\u001b[38;5;241m.\u001b[39mdesired_pose \u001b[38;5;241m-\u001b[39m pose) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.01\u001b[39m:\n",
      "File \u001b[1;32mh:\\Courses_files\\Master\\2023_Fall\\drone\\SpecialCourseDrone\\Dronesimscape.py:153\u001b[0m, in \u001b[0;36mDronesimscape.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m\"\"\"Method for stepping the simulation.\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput2action(action)\n\u001b[1;32m--> 153\u001b[0m state, simulation_time, terminated, truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesired_pose))\n\u001b[0;32m    155\u001b[0m axispose \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\simulink_gym-0.6.1-py3.10.egg\\simulink_gym\\environment.py:189\u001b[0m, in \u001b[0;36mSimulinkEnv.sim_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_data(np\u001b[38;5;241m.\u001b[39marray(action))\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Receive data:\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m recv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# When the simulation is truncated an empty message is received:\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m recv_data:\n",
      "File \u001b[1;32md:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\simulink_gym-0.6.1-py3.10.egg\\simulink_gym\\utils\\comm_socket.py:78\u001b[0m, in \u001b[0;36mCommSocket.receive\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"Method for receiving data from the simulation.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    raw data received over the socket\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected():\n\u001b[1;32m---> 78\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     data_array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m, data)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_array\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = \"Dronesimscape\"\n",
    "has_continuous_action_space = True\n",
    "\n",
    "max_ep_len = 5000              # max timesteps in one episode\n",
    "max_training_timesteps = int(1e8)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = None\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## Note : print/log frequencies should be > than max_ep_len\n",
    "\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"agent/PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim=19, action_dim=10, lr_actor = lr_actor, lr_critic = lr_critic, gamma = gamma, K_epochs = K_epochs, eps_clip = eps_clip, has_continuous_action_space = True, action_std_init=0.1)\n",
    "ppo_agent.load(checkpoint_path)\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "action_std_decay_freq = int(1e5)\n",
    "action_std_decay_rate = 1e-3\n",
    "min_action_std = 1e-3\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        pose = np.array([state[3], state[5], state[7]])\n",
    "        if np.linalg.norm(env.desired_pose - pose) < 0.01:\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "env.close()\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test agent:\n",
    "trajectory = []\n",
    "sim_time = []\n",
    "actions = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    trajectory.append(state)\n",
    "    sim_time.append(info[\"simulation time [s]\"])\n",
    "    actions.append(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the current episode:\n",
    "env.stop_simulation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environment:\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "713853fc4eb821a20254f41f6276f3723ba35eac7488bde8c3eee95c0a6b7c43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
