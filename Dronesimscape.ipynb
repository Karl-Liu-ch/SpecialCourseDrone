{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dronesimscape import Dronesimscape\n",
    "from simulink_gym import logger\n",
    "\n",
    "logger.setLevel(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting Matlab engine\n",
      "INFO: Adding components to Matlab path\n",
      "INFO: Creating simulation input object for model Dronesimscape.slx\n"
     ]
    }
   ],
   "source": [
    "# Create training environment:\n",
    "env = Dronesimscape(stop_time=10, step_size=0.001, timestep=0.001, model_debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: recv_socket: Socket not connected, nothing to close\n",
      "INFO: send_socket: Socket not connected, nothing to close\n"
     ]
    }
   ],
   "source": [
    "# Reset environment:\n",
    "state = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute single step in the environment:\n",
    "# action = env.action_space.sample()\n",
    "# state, reward, done, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = env.action_space.sample()\n",
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Device set to : NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from agent.PPOAgent import *\n",
    "total_test_episodes = 10    # total num of testing episodes\n",
    "\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003           # learning rate for actor\n",
    "lr_critic = 0.001           # learning rate for critic\n",
    "env_name = 'Dronesimscape'\n",
    "directory = \"agent/PPO_preTrained/\" + env_name + '/'\n",
    "checkpoint_path = directory + \"PPO_{}.pth\".format(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "training environment name : Dronesimscape\n",
      "save checkpoint path : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  100000000\n",
      "max timesteps per episode :  1000\n",
      "model saving frequency : 20000 timesteps\n",
      "log frequency : 2000 timesteps\n",
      "printing average reward over episodes in last : 4000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a continuous action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "starting std of action distribution :  None\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 4000 timesteps\n",
      "PPO K epochs :  40\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Started training at (GMT) :  2023-12-11 19:38:26\n",
      "============================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: recv_socket: Socket not connected, nothing to close\n",
      "INFO: send_socket: Socket not connected, nothing to close\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 3 \t\t Timestep : 4000 \t\t Average Reward : -5172.48\n",
      "Episode : 7 \t\t Timestep : 8000 \t\t Average Reward : -4090.63\n",
      "Episode : 11 \t\t Timestep : 12000 \t\t Average Reward : -4078.38\n",
      "Episode : 15 \t\t Timestep : 16000 \t\t Average Reward : -4377.57\n",
      "Episode : 19 \t\t Timestep : 20000 \t\t Average Reward : -4353.27\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:07:37\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 23 \t\t Timestep : 24000 \t\t Average Reward : -4117.56\n",
      "Episode : 27 \t\t Timestep : 28000 \t\t Average Reward : -4049.31\n",
      "Episode : 31 \t\t Timestep : 32000 \t\t Average Reward : -4101.23\n",
      "Episode : 35 \t\t Timestep : 36000 \t\t Average Reward : -3827.77\n",
      "Episode : 39 \t\t Timestep : 40000 \t\t Average Reward : -3064.91\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:15:07\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 43 \t\t Timestep : 44000 \t\t Average Reward : -5551.63\n",
      "Episode : 47 \t\t Timestep : 48000 \t\t Average Reward : -15977.82\n",
      "Episode : 51 \t\t Timestep : 52000 \t\t Average Reward : -9169.06\n",
      "Episode : 55 \t\t Timestep : 56000 \t\t Average Reward : -8041.06\n",
      "Episode : 59 \t\t Timestep : 60000 \t\t Average Reward : -9032.17\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:22:45\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 63 \t\t Timestep : 64000 \t\t Average Reward : -8244.62\n",
      "Episode : 67 \t\t Timestep : 68000 \t\t Average Reward : -8193.43\n",
      "Episode : 71 \t\t Timestep : 72000 \t\t Average Reward : -7693.18\n",
      "Episode : 75 \t\t Timestep : 76000 \t\t Average Reward : -7194.08\n",
      "Episode : 79 \t\t Timestep : 80000 \t\t Average Reward : -7434.43\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:30:37\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 83 \t\t Timestep : 84000 \t\t Average Reward : -6332.82\n",
      "Episode : 87 \t\t Timestep : 88000 \t\t Average Reward : -6203.99\n",
      "Episode : 91 \t\t Timestep : 92000 \t\t Average Reward : -5387.46\n",
      "Episode : 95 \t\t Timestep : 96000 \t\t Average Reward : -4602.51\n",
      "Episode : 99 \t\t Timestep : 100000 \t\t Average Reward : -4854.36\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:38:27\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 103 \t\t Timestep : 104000 \t\t Average Reward : -5123.73\n",
      "Episode : 107 \t\t Timestep : 108000 \t\t Average Reward : -3986.86\n",
      "Episode : 111 \t\t Timestep : 112000 \t\t Average Reward : -4289.11\n",
      "Episode : 115 \t\t Timestep : 116000 \t\t Average Reward : -3927.12\n",
      "Episode : 119 \t\t Timestep : 120000 \t\t Average Reward : -3680.51\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:45:53\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 123 \t\t Timestep : 124000 \t\t Average Reward : -3329.18\n",
      "Episode : 127 \t\t Timestep : 128000 \t\t Average Reward : -2329.72\n",
      "Episode : 131 \t\t Timestep : 132000 \t\t Average Reward : -3636.11\n",
      "Episode : 135 \t\t Timestep : 136000 \t\t Average Reward : -3519.8\n",
      "Episode : 139 \t\t Timestep : 140000 \t\t Average Reward : -2184.39\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:53:10\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 143 \t\t Timestep : 144000 \t\t Average Reward : -2133.5\n",
      "Episode : 147 \t\t Timestep : 148000 \t\t Average Reward : -1911.74\n",
      "Episode : 151 \t\t Timestep : 152000 \t\t Average Reward : -2008.09\n",
      "Episode : 155 \t\t Timestep : 156000 \t\t Average Reward : -1780.9\n",
      "Episode : 159 \t\t Timestep : 160000 \t\t Average Reward : -1805.81\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  1:00:32\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 163 \t\t Timestep : 164000 \t\t Average Reward : -1785.38\n",
      "Episode : 167 \t\t Timestep : 168000 \t\t Average Reward : -1753.87\n",
      "Episode : 171 \t\t Timestep : 172000 \t\t Average Reward : -1724.46\n",
      "Episode : 175 \t\t Timestep : 176000 \t\t Average Reward : -1748.29\n",
      "Episode : 179 \t\t Timestep : 180000 \t\t Average Reward : -1756.13\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  1:07:47\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 183 \t\t Timestep : 184000 \t\t Average Reward : -1613.01\n",
      "Episode : 187 \t\t Timestep : 188000 \t\t Average Reward : -1549.71\n",
      "Episode : 191 \t\t Timestep : 192000 \t\t Average Reward : -1528.23\n",
      "Episode : 195 \t\t Timestep : 196000 \t\t Average Reward : -1479.52\n",
      "Episode : 199 \t\t Timestep : 200000 \t\t Average Reward : -1448.01\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : agent/PPO_preTrained/Dronesimscape/PPO_Dronesimscape_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  1:15:33\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = \"Dronesimscape\"\n",
    "has_continuous_action_space = True\n",
    "\n",
    "max_ep_len = 1000              # max timesteps in one episode\n",
    "max_training_timesteps = int(1e8)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = None\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## Note : print/log frequencies should be > than max_ep_len\n",
    "\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"agent/PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim=19, action_dim=10, lr_actor = lr_actor, lr_critic = lr_critic, gamma = gamma, K_epochs = K_epochs, eps_clip = eps_clip, has_continuous_action_space = True, action_std_init=0.1)\n",
    "ppo_agent.load(checkpoint_path)\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "action_std_decay_freq = int(1e5)\n",
    "action_std_decay_rate = 1e-3\n",
    "min_action_std = 1e-3\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        pose = np.array([state[3], state[5], state[7]])\n",
    "        if np.linalg.norm(env.desired_pose - pose) < 0.01:\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "env.close()\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test agent:\n",
    "trajectory = []\n",
    "sim_time = []\n",
    "actions = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    trajectory.append(state)\n",
    "    sim_time.append(info[\"simulation time [s]\"])\n",
    "    actions.append(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the current episode:\n",
    "env.stop_simulation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environment:\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "713853fc4eb821a20254f41f6276f3723ba35eac7488bde8c3eee95c0a6b7c43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
